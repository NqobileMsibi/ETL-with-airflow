{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Moviedata\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cecc885",
   "metadata": {},
   "source": [
    "# User purchase data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec60060",
   "metadata": {},
   "outputs": [],
   "source": [
    "Userdf = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"user_purchase.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce77f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values\n",
    "\n",
    "Userdf.select([F.count(F.when(F.isnull(x), x)).alias(x) for x in Userdf.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dee2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null values\n",
    "dropped_null_df = Userdf.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again for null values\n",
    "\n",
    "dropped_null_df.select(\n",
    "    [F.count(F.when(F.isnull(x), x)).alias(x) for x in dropped_null_df.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e7b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate values\n",
    "\n",
    "new_df = dropped_null_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dc1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for negative values\n",
    "for column in new_df.columns:\n",
    "    neg_values = new_df.select(F.col(column)).where(F.col(column) < 0).count()\n",
    "    print(f\"{column}-\\n\\t Negative values: {neg_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5564bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove negative values\n",
    "\n",
    "df_without_negative_values = new_df.where(\n",
    "    (F.col(\"Quantity\") >= 0) & (F.col(\"UnitPrice\") >= 0)\n",
    ")\n",
    "\n",
    "# I don't think Quantity should have a zero value. So we can change the\n",
    "# condition on Quatity to be only > and not >= before running the code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for negative values again\n",
    "\n",
    "for column in df_without_negative_values.columns:\n",
    "    neg_values = (\n",
    "        df_without_negative_values.select(F.col(column))\n",
    "        .where(F.col(column) < 0)\n",
    "        .count()\n",
    "    )\n",
    "    print(f\"{column}-\\n\\t Negative values: {neg_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new clean file\n",
    "df_without_negative_values.write.option(\"header\",True).csv(\"user_purchase_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a1892",
   "metadata": {},
   "source": [
    "# Movie Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f430f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "moviedf = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"movie_review.csv\")\n",
    ")\n",
    "\n",
    "# df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Getting a list of words\n",
    "tokenizer = Tokenizer(inputCol=\"review_str\", outputCol=\"review_token\")\n",
    "moviedf = tokenizer.transform(moviedf)\n",
    "# moviedf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088a8e01-71a9-4ca3-b95b-6a60c7e77402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words\n",
    "remover = StopWordsRemover(inputCol=\"review_token\", outputCol=\"filtered\")\n",
    "moviedf = remover.transform(moviedf)\n",
    "# moviedf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7163e8-e90d-4a1b-9a77-573f5f567989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting good words\n",
    "moviedf = moviedf.withColumn(\n",
    "    \"positive_review\", array_contains(moviedf[\"filtered\"], \"good\")\n",
    ")\n",
    "moviedf = moviedf.withColumn(\"insert_column\", current_timestamp())\n",
    "# moviedf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385d726-09ca-4197-bc3e-42ef851cd9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverting boolean true and false to 0 and 1\n",
    "moviedf = moviedf.withColumn(\n",
    "    \"positive_review_int\", when(moviedf[\"positive_review\"] == \"true\", 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# dropping columns\n",
    "moviedf = moviedf.drop(\n",
    "    \"review_str\", \"review_token\", \"filtered\", \"positive_review\", \"insert_column\"\n",
    ")\n",
    "# moviedf.show(10)\n",
    "\n",
    "# new clean file\n",
    "moviedf.write.option(\"header\",True).csv(\"movie_review_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d9508",
   "metadata": {},
   "source": [
    "# Log Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62241f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "logdf = spark.read.option(\"header\", \"true\").csv(\"log_reviews.csv\")\n",
    "\n",
    "# creating the Schema\n",
    "xmlSchema = StructType(\n",
    "    [\n",
    "        StructField(\"logDate\", StringType(), True),\n",
    "        StructField(\"device\", StringType(), True),\n",
    "        StructField(\"location\", StringType(), True),\n",
    "        StructField(\"os\", StringType(), True),\n",
    "        StructField(\"ipAddress\", StringType(), True),\n",
    "        StructField(\"phoneNumber\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# extract log column from dataframe\n",
    "xml_data = logdf.select(\"log\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Convert the XML data to an RDD\n",
    "xml_rdd = spark.sparkContext.parallelize(xml_data)\n",
    "\n",
    "\n",
    "def parse_xml(xml_string):\n",
    "    root = ET.fromstring(xml_string)\n",
    "    logs = []\n",
    "    for log in root.findall(\"log\"):\n",
    "        log_dict = {}\n",
    "        log_dict[\"logDate\"] = log.find(\"logDate\").text\n",
    "        log_dict[\"device\"] = log.find(\"device\").text\n",
    "        log_dict[\"location\"] = log.find(\"location\").text\n",
    "        log_dict[\"os\"] = log.find(\"os\").text\n",
    "        log_dict[\"ipAddress\"] = log.find(\"ipAddress\").text\n",
    "        log_dict[\"phoneNumber\"] = log.find(\"phoneNumber\").text\n",
    "        logs.append(log_dict)\n",
    "    return logs\n",
    "\n",
    "# parse_xml function to each element of the XML RDD\n",
    "parsed_xml_rdd = xml_rdd.flatMap(parse_xml)\n",
    "\n",
    "# creating a new data frame\n",
    "df = spark.createDataFrame(parsed_xml_rdd, schema=xmlSchema)\n",
    "\n",
    "# creating a new column with the id review\n",
    "df = df.withColumn(\"id_review\", monotonically_increasing_id() + 1)\n",
    "df.show()\n",
    "df.write.option(\"header\",True).csv(\"log_reviews_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8098598d-198f-4967-9e70-169c48c38a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_movie = spark.read.option(\"header\", \"true\").csv(\"movie_review_clean.csv\")\n",
    "new_movie.show(10)\n",
    "\n",
    "new_log = spark.read.option(\"header\", \"true\").csv(\"log_reviews_clean.csv\")\n",
    "new_log.show(10)\n",
    "\n",
    "new_user = spark.read.option(\"header\", \"true\").csv(\"user_purchase_clean.csv\")\n",
    "new_user.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
